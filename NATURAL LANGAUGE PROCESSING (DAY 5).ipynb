{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGAUGE PROCESSING (DAY 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TOOLS THAT USED FOR TOKENIZATION\n",
    "- SENTENCE TOKENIZATION \n",
    "- WORD TOKENIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 SENTENCE TOKENIZATION METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Tool Kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Tokenization is the process of breaking down a piece of text into small units called tokens. A token may be a word, part of a word or just characters like punctuation.\"\"\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is the process of breaking down a piece of text into small units called tokens.',\n",
       " 'A token may be a word, part of a word or just characters like punctuation.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using re (REGEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is the process of breaking down a piece of text into small units called tokens',\n",
       " 'A token may be a word, part of a word or just characters like punctuation.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.compile('[.!?] ').split(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91799\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91799\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91799\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of breaking down a piece of text into small units called tokens.\n",
      "A token may be a word, part of a word or just characters like punctuation.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc= nlp(sentence)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenization is the process of breaking down a piece of text into small units called tokens',\n",
       " ' a token may be a word, part of a word or just characters like punctuation']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text_to_word_sequence(sentence, split=\".\", filters =\"!.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is the process of breaking down a piece of text into small units called tokens.',\n",
       " 'A token may be a word, part of a word or just characters like punctuation.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences\n",
    "list(split_sentences(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 WORD TOKENIZATION METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'piece', 'of', 'text', 'into', 'small', 'units', 'called', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Tokenization is the process of breaking down a piece of text into small units called tokens')\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'text',\n",
       " 'into',\n",
       " 'small',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Tokenization is the process of breaking down a piece of text into small units called tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'text',\n",
       " 'into',\n",
       " 'small',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens.',\n",
       " 'A',\n",
       " 'token',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'word',\n",
       " ',',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'or',\n",
       " 'just',\n",
       " 'characters',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "  \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'text',\n",
       " 'into',\n",
       " 'small',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens.',\n",
       " 'A',\n",
       " 'token',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'word,',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'or',\n",
       " 'just',\n",
       " 'characters',\n",
       " 'like',\n",
       " 'punctuation.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGEX METHOD - REGULAR EXPRESSION TOKENIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "    - Refer this https://regex101.com/ site to try or write regex patterns on our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'text',\n",
       " 'into',\n",
       " 'small',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " 'A',\n",
       " 'token',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'word',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'or',\n",
       " 'just',\n",
       " 'characters',\n",
       " 'like',\n",
       " 'punctuation']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[\\w]+', sentence) #https://regex101.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'text',\n",
       " 'into',\n",
       " 'small',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " 'a',\n",
       " 'token',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'word',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'or',\n",
       " 'just',\n",
       " 'characters',\n",
       " 'like',\n",
       " 'punctuation']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'text',\n",
       " 'into',\n",
       " 'small',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " 'A',\n",
       " 'token',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'word',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'or',\n",
       " 'just',\n",
       " 'characters',\n",
       " 'like',\n",
       " 'punctuation']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "list(tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://www.analyticsvidhya.com/ , https://medium.com/ , https://www.geeksforgeeks.org/ , https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGAUGE PROCESSING (DAY 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STEMMING\n",
    "- LEMMATIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are algorithms that are used in Natural Language Processing (NLP) to normalize text and prepare words and documents for further processing in Machine Learning. In NLP, for example, you may want to acknowledge the fact that the words “like” and “liked” are the same word in different tenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
